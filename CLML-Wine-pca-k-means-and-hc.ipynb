{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLML, Wine, PCA, K-Means and Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (C) 2015 Mike Maul -- CC-BY-SA 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is part series of tutorials illustrating the use of CLML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Caveat\n",
    "Anyone wishing to run this notebook or the code contained there in must take note of the following:\n",
    "  - This tutorial relies on the github version of [`CLML` https://github.com/mmaul/clml.git](https://github.com/mmaul/clml.git) or a quicklist-dist `CLML`> than 20150805 \n",
    "  - The plotting portion of this code requires the system [`clml.extras` https://github.com/mmaul/clml.extras.git](https://github.com/mmaul/clml.extras.git) which is not currently in quicklisp.\n",
    "  - While the above git repositories are not in quicklisp they be loaded by `quickload` by placing the repositories in $HOME/quicklisp/local-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This article will discuss two clustering techniques, k-means and Hierarchical Clustering.\n",
    "Clustering is an unsupervised learning technique, with the goa to group to samples into a given number of partitions.\n",
    "Clustering uses uses the similarity between examples and groups examples based of their mutual similarities.\n",
    "\n",
    "## Wine\n",
    "\n",
    "Wine such a multideminensional beverage. A feast for the senses, taste, smell even for the eyes. It also pairs well with statistical analysis techniques and even Lisp. What does wine and statistical analysis have to do with one another you may ask. Well the Wine dataset at it the 'goto' dataset used in just about every introduction cluster analysis. The Wine dataset is a chemical analysis of three types of wines wines grown in a region of Italy. The dataset contains an analysis of 178 samples, with 13 results of chemical assays for each sample. It is small enough yet contains enough complexity to be interesting. The real value is that the dataset already contains the classification, so the lazy (er I mean pragmitic) presenters don't have to justify the classification, they only have to compare the results agains the esisting classifications. While were on the subject of wine if you find yourself in eastern Washington, I highly recommend stopping by the [Parisisos del Sol](www.paradisosdelsol.com) winery, good wine, down to earth atmosphere and a really interesting and knowledgeable propriter. \n",
    "\n",
    "But back to the cluster analysis stuff...\n",
    "\n",
    "Lets get started by loading the system necessary for this tutorial and creating a namespace to work in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To load \"clml.utility\":\n",
      "  Load 1 ASDF system:\n",
      "    clml.utility\n",
      "\n",
      "; Loading \"clml.utility\"\n",
      "....\n",
      "To load \"clml.hjs\":\n",
      "  Load 1 ASDF system:\n",
      "    clml.hjs\n",
      "\n",
      "; Loading \"clml.hjs\"\n",
      "\n",
      "To load \"clml.pca\":\n",
      "  Load 1 ASDF system:\n",
      "    clml.pca\n",
      "\n",
      "; Loading \"clml.pca\"\n",
      "\n",
      "To load \"clml.clustering\":\n",
      "  Load 1 ASDF system:\n",
      "    clml.clustering\n",
      "\n",
      "; Loading \"clml.clustering\"\n",
      "\n",
      "To load \"iolib\":\n",
      "  Load 1 ASDF system:\n",
      "    iolib\n",
      "\n",
      "; Loading \"iolib\"\n",
      ".....\n",
      "To load \"clml.extras.eazy-gnuplot\":\n",
      "  Load 1 ASDF system:\n",
      "    clml.extras.eazy-gnuplot\n",
      "\n",
      "; Loading \"clml.extras.eazy-gnuplot\"\n",
      "\n",
      "To load \"eazy-gnuplot\":\n",
      "  Load 1 ASDF system:\n",
      "    eazy-gnuplot\n",
      "\n",
      "; Loading \"eazy-gnuplot\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "#<PACKAGE \"WINE\">"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(progn\n",
    "    (ql:quickload '(:clml.utility ; Need clml.utility.data to get data from the net\n",
    "                :clml.hjs ; Need clml.hjs.read-data to poke around the raw dataset\n",
    "                :clml.pca\n",
    "                :clml.clustering\n",
    "                :iolib\n",
    "                :clml.extras.eazy-gnuplot\n",
    "                :eazy-gnuplot\n",
    "                ))\n",
    "    (defpackage #:wine\n",
    "      (:use #:cl\n",
    "        #:cl-jupyter-user\n",
    "        #:clml.hjs.read-data\n",
    "        #:clml.utility.data\n",
    "        #:clml.hjs.vector\n",
    "        #:clml.hjs.matrix\n",
    "        #:clml.hjs.k-means\n",
    "        #:clml.pca\n",
    "        #:clml.clustering.hc\n",
    "        #:eazy-gnuplot\n",
    "        ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#<PACKAGE \"WINE\">"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(in-package :wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Take time to get to know your wine\n",
    "\n",
    "This tutorial illistrates how to use clustering, as well as how to use Principal Componen Analysis in support of k-means and also hierarchical clustering in CLML. Just like swilling a bottle of wine is not a good thing neither is turning losse analysis techniques willy nilly on data. It is necessary to understand your data first, as the saying goes, garbage in garbage out...\n",
    "\n",
    "Another take away from this tutorial is working with diverse data. Our dataset comes UCI Machine learning archive, the convention there is a headerless CSV file with seperate file ending in `.names` containing the a description of the dataset and the column names.\n",
    "\n",
    "So armed with some information about the origin and meaning of the data, the orignization and the location we can begin.\n",
    "The dataset is located at http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine\n",
    "\n",
    "Well almost ready another thing that is necessary is to asses the data so we can ingest it properly. This is something that generally needs to be done manually or can be skipped with prior knowldege of the dataset. \n",
    "\n",
    "The `wine.names` file is rather verbose so We will just list the column names:\n",
    "   1. Alcohol\n",
    "   2. Malic acid\n",
    "   3. Ash\n",
    "   4. Alcalinity of ash  \n",
    "   5. Magnesium\n",
    "   6. Total phenols\n",
    "   7. Flavanoids\n",
    "   8. Nonflavanoid phenols\n",
    "   9. Proanthocyanins\n",
    "   10. Color intensity\n",
    "   11. Hue\n",
    "   12. OD280/OD315 of diluted wines\n",
    "   13. Proline            \n",
    "\n",
    "So lets take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UCI-WINE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(defparameter uci-wine \n",
    "    (read-data-from-file\n",
    "        (fetch \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\")\n",
    "        :type :csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#(#(1 13.2 1.78 2.14 11.2 100 2.65 2.76 .26 1.28 4.38 1.05 3.4 1050)\n",
       "  #(1 13.16 2.36 2.67 18.6 101 2.8 3.24 .3 2.81 5.68 1.03 3.17 1185)\n",
       "  #(1 14.37 1.95 2.5 16.8 113 3.85 3.49 .24 2.18 7.8 .86 3.45 1480)\n",
       "  #(1 13.24 2.59 2.87 21 118 2.8 2.69 .39 1.82 4.32 1.04 2.93 735)\n",
       "  #(1 14.2 1.76 2.45 15.2 112 3.27 3.39 .34 1.97 6.75 1.05 2.85 1450))\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(format nil \"~A\" (head-points uci-wine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So is is all numeric data, The first colum from the dataset definition is the class so we don't want to give that to our\n",
    "clustering algorithim as that would be cheating. The numeric collumns should be of type double float\n",
    "\n",
    "The k-means, and hierichal clustering implementationsrequires a numeric-dataset which we create by using `pick-and-specialize data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WINE-WITH-CLASSIFICATIONS"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(let ((wine-unspecialized (read-data-from-file\n",
    "    (fetch \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\")\n",
    "    :type :csv\n",
    "    :csv-type-spec '(integer\n",
    "                     double-float double-float  double-float\n",
    "                     double-float double-float  double-float\n",
    "                     double-float double-float  double-float\n",
    "                     double-float double-float  double-float\n",
    "                     double-float)\n",
    "    :csv-header-p ( list \"Class\"\n",
    "                         \"Alcohol\"            \"Malic acid\"           \"Ash\"\n",
    "                         \"Alcalinity of ash\"  \"Magnesium\"            \"Total phenols\"\n",
    "                         \"Flavanoids\"         \"Nonflavanoid phenols\" \"Proanthocyanins\"\n",
    "                         \"Color intensity\"    \"Hue\"                   \"OD280/OD315 of diluted wines\"\n",
    "                         \"Proline\")\n",
    "    )))\n",
    "  (defparameter wine\n",
    "    (pick-and-specialize-data \n",
    "       wine-unspecialized   \n",
    "       :range '(1 2 3 4 5 6 7 8 9 10 11 12 13) :data-types (make-list 13 :initial-element :numeric )))\n",
    "  (defparameter wine-with-classifications\n",
    "    (pick-and-specialize-data \n",
    "       wine-unspecialized   \n",
    "       :data-types (make-list 14 :initial-element :numeric )))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Letting the wine breathe\n",
    "The process of letting wine breathe is to exposure to the surrounding air. Letting allowing wine to mix and mingle with air and surrounding tempature. Similarly our data needs to \"mix and mingle\" so that the scales become more uniform which is helpful to clustering algorithims.\n",
    "\n",
    "So we need to consider if the data needs to be standardized, generally the answer will be yes, unless the \n",
    "columns are all of the same units and have lov variance. Specifically with this datasets since the columns are using different units scaling is necessary. Our our implementation of `k-means` can do our standardization for us however we will need to do it for the principal component analysis.  The `standardize` function used here will be the z-score  which is $(c - \\mu) / \\sigma$.\n",
    "\n",
    "## Savoring the wine\n",
    "Savoring ths the process of detecting the essential properties generally using taste, we whiil however use principal component analysis instead. \n",
    "\n",
    "So what columns should be evaluated? We could evaluate all of them that might be okay, and would work in this case. However in the case of large datasets it may be necessary to reduce the dimensionality to decrease execution time. It could also be the case that some relatively unimportant columns could be throwing off the results.\n",
    "\n",
    "#### Principal Component Analysis\n",
    "Well one technique we can use to select the relevant columns for analysis is principal component analysis. Principal Component Analysis is a set of orthagonal transformations that reduce dimensionality in a dataset. One of the things that PCA can tell us is the mangnitute of the contribution of each column to the variation in the data. \n",
    "\n",
    "PCA is one of those algrorithims that needs regular data so we are going to standardize the dataset first. \n",
    "\n",
    "Lets compute a few things we will be needing.\n",
    "  - `standardized-wine`: The stadardized copy of wine dataset\n",
    "  - `pca-result`: The result of the PCA analysis of the standardized wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA-RESULT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(progn \n",
    "  (defparameter standardized-wine (copy-dataset wine))\n",
    "  (setf (dataset-numeric-points standardized-wine) (standardize (dataset-numeric-points standardized-wine)))\n",
    "  \n",
    "  (defparameter pca-result (princomp standardized-wine))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods of evaluating the choice of principal components is the 'elbow' method. The elbow method involves choosing the number of components that lie in the elbow of the graph. In the graph below three or four would be a reasonable choice. \n",
    "\n",
    "Specifically `contributions` returns the variances of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "#<CL-JUPYTER-USER::PNG-BYTES {10088EC0F3}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(let ((png \"wine-pca-contributions.png\"))\n",
    "    (clml.extras.eazy-gnuplot::plot-series (contributions pca-result) \n",
    "    :term '(:png) :output png :plot-title \"PCA Contributions\" :ylabel \"Variance\" :xlabel \"Column\" :series-title \"\")\n",
    "    (display-png (png-from-file png)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the primary uses of PCA is to reduce dimensionality of data. In this case it allows us to make the decision that first four columns are what drive the majority of the variation in the data. So knowing this we will be truncating the data set to four columns. While this is not that critical in the current dataset which has only thirteen dimensions, it can be critical when dealing with datasets with hundreds or thousands of columns. \n",
    "\n",
    "So lets partition off the dataset so it only contains the principal columns, as we will need that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRUNCATED-STANDARDIZED-WINE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(defparameter truncated-standardized-wine (make-numeric-dataset  \n",
    "                       (map 'list #'dimension-name (subseq (dataset-dimensions wine) 0 4))                           \n",
    "                       (map 'vector (lambda (r) (subseq r 0 4)) (dataset-points standardized-wine))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the wine tasting, a blind tasting\n",
    "So for wine to be judged impartially it should be served without context and evaluated solely on its own merits. This is really what we are doing we already stripped the classifications from the dataset, and we aren't even tasting the wines ourselves. Out goal is to try match the wine varietal type with the preexisting classifications.\n",
    "\n",
    "#### Deciding on the clusters\n",
    "We need to have some idea of how the data will be grouped. In some cases this will be known, in others it will not. In this case we know theere are three different classifications.\n",
    "\n",
    "But what if we didn't know? \n",
    "\n",
    "One method is to look at the within group of sums of squares among the contributions of the principal components of the dataset. We calculate the first value storing it in `wss1` the remaining values can be calulated by taking the sum of squares of the distances returned from running k-means (as the `distance-between-point-and-owner` slot in the `pca-result` instance returned from k-means)on the selected principal components from the previous PCA analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "#<CL-JUPYTER-USER::PNG-BYTES {100A0733F3}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(let ((wss1 (* (- (length (components pca-result)) 1) \n",
    "               (loop for v across (subseq (contributions pca-result) 0 4) sum v)))\n",
    "       (comp-ds (make-numeric-dataset '(\"pc1\" \"pc2\" \"pc3\" \"pc4\")\n",
    "                                      (map 'vector (lambda (r) (subseq r 0 4)) (components pca-result))))\n",
    "        (png \"group-sum-of-squares.png\"))\n",
    "    (clml.extras.eazy-gnuplot::plot-series \n",
    "      (coerce (cons wss1 \n",
    "                  (loop for n from 2 upto 8 ; could be up size of dimensions the far end is generally irrelevant\n",
    "                        for k-means-n = (k-means n comp-ds  :standardization nil \n",
    "                                                 :random-state (make-random-state-with-seed 100))\n",
    "                        collect (loop for x across (clml.hjs.k-means::pw-distance-between-point-and-owner k-means-n) \n",
    "                                      sum (* x x)))) \n",
    "               'vector)\n",
    "       :term '(:png) :output png :plot-title \"Group Sum of Squares\" :ylabel \"In group sum of squares\" \n",
    "       :xlabel \"Clusters\" :series-title \"\")\n",
    "  \n",
    "  (display-png (png-from-file png)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use the \"elbow\" method to make our selection (additionally in either selecting number of cluseter or the pricinpal components we can use the 85% precentile to make the choice point).\n",
    "\n",
    "But what is really cool is the elbow lies right on three cluster which matches the number of clusters that had been manually classified in the wine dataset. \n",
    "\n",
    "#### K-means\n",
    "So we kind of jumped into using `k-means` before talking about it...\n",
    "Simplified description of the K-means Algorithm:\n",
    "\n",
    "1. Initial cluster are randomly chosen.\n",
    "2. The squared distance from each object to each cluster is computed, then the objects are assigned to the nearest cluster.\n",
    "3. New centroid are computed for each cluster â€“ and each cluster is  by the respective cluster centroid.\n",
    "4. The squared distance from each object to each cluster is computed, and the objects are assigned to the cluster nearest cluster with the smallest squared distance.\n",
    "5. Based upon the new membership assignement cluster centroids are recalculated.\n",
    "6. Steps 4 and 5 are repeated until stability is achived.\n",
    "\n",
    "The k in k-means is the number of clusters that k-means will be classifing. K is required parameter in k-means but how should we decide what to set it to?  Well we already know from the original dataset that there are three clusters.\n",
    "\n",
    "So for testing or teaching you will want to set a fixed `:random-state` which will allow you have repeatable results (It influnces the choice of the seed clusters). \n",
    "But keep in mind if you set it all the time you might get caught on a local optima that you might have avoided if \n",
    "you had allowed the state to be random.\n",
    "\n",
    "The CLML implementation of k-means accepts a number of parameters for running k-means.\n",
    "\n",
    "* k-means arguments *\n",
    "    - k:              <integer>, number of clusters\n",
    "    - dataset:        <numeric-dataset> | <category-dataset> | <numeric-or-category-dataset>\n",
    "    - distance-fn:    #'euclid-distance | #'manhattan-distance | #'cosine-distance\n",
    "    - standardization:t | nil, whether to standardize the inputs\n",
    "    - max-iteration:  maximum number of iterations of one trial (default 1000)\n",
    "    - num-of-trials:  number of trials, every trial changes the initial position of the clusters. (Default 10)\n",
    "    - random-state:   (for testing), specify the random-state of the random number generator\n",
    "    - debug:          (for debugging) print out some debugging information\n",
    "\n",
    "* k-means results *\n",
    "    - workspacet: points, clusters, distance infomation, etc.\n",
    "    - table:      lookup table for normalized vecs and original vecs, might be removed later.\n",
    "    \n",
    "Okay, so now that we know the number of clusters (well we've always known, but we also arrived at the same choice on our own), lets run k-means on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#<CLML.HJS.K-MEANS::PROBLEM-WORKSPACE 3 Clusters (ID size): ((0 54) (1 62)\n",
       "                                                             (2 62)) {1007736283}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (progn  (defparameter workspace nil) \n",
    "         (defparameter table nil)\n",
    "         (multiple-value-setq (workspace table)\n",
    "           (k-means 3 truncated-standardized-wine  :standardization nil \n",
    "                    :random-state (make-random-state-with-seed 1234))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see out cluster assigment in the pretty print of the object. Lets compare roughly with the counts of the classification that came with the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1 59) (2 71) (3 48))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loop for c across (!! wine-with-classifications \"Class\") \n",
    "  when (= c 1.0) count c into one when (= c 2.0) count c into two when (= c 3.0) count c into three \n",
    "  finally (return (list (list 1  one) (list 2 two) (list 3 three))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that doesn's look all that bad We can verify this in a bit.\n",
    "\n",
    "One thing that hasn't been addressed is how to associate the classifications with the un-standardized data. Earlier we created a parameter `table` table contains a hash table that associates the standardized rows with their unstandardized rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "\n",
    "###### How it works\n",
    "\n",
    "The clml imlementation of Hierarchical Clustering uses agglomerative clustering (bottom up approach)\n",
    "A nice concise description of the processs of agglomerative clustering from [improvedoutcomes.com/](http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Agglomerative_Hierarchical_Clustering_Overview.htm) is as follows:\n",
    "  - Assign each object to a separate cluster.\n",
    "  - Evaluate all pair-wise distances between clusters (distance metrics are described in Distance Metrics Overview).\n",
    "  - Construct a distance matrix using the distance values.\n",
    "  - Look for the pair of clusters with the shortest distance.\n",
    "  - Remove the pair from the matrix and merge them.\n",
    "  - Evaluate all distances from this new cluster to all other clusters, and update the matrix.\n",
    "  - Repeat until the distance matrix is reduced to a single element.\n",
    "\n",
    "The `distance-matrix`takes an optional argument `:distance-fn` which specifices the distance function used in the distance computations. The distance functions avalable are:\n",
    "  - ecludian (Default)\n",
    "  - manhattan\n",
    "  - pearson\n",
    "  - cosine\n",
    "  - tanimoto\n",
    "  - caberra\n",
    "\n",
    "The `cophonetic-matric` function evaluates the distance matric and returns the merge and cophonetic matrix. The `cophonetic-matrix` take a optional `:method` parameter to specify the method used to evaulate the distance matrix. The evaluation methods are as follows.\n",
    "  - `hc-single`\n",
    "  - `hc-complete`\n",
    "  - `hc-average` (default)\n",
    "  - `hc-centroid`\n",
    "  - `hc-median`\n",
    "  - `hc-ward`\n",
    "\n",
    "`cuttree` cuts the tree defined in the merge matrix into the specified number of pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut tree: #(1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "            1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 2 2 3 3 3 1 3 3\n",
      "            2 1 3 1 3 1 3 3 3 3 1 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 1 2 3 3 3 3 3\n",
      "            3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 1 3 3 3 3 3 3 3 3 2 2 2 2 2 2\n",
      "            2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "            2 2 2 2 2 2 2 2) \n",
      "Class counts:((1 65) (2 54) (3 59)) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NIL"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(progn\n",
    "  (defparameter distance-matrix (distance-matrix (numeric-matrix standardized-wine)))\n",
    "  (defparameter u nil) (defparameter v nil)\n",
    "  (multiple-value-setq (u v) (cophenetic-matrix distance-matrix #'hc-ward))\n",
    "  (defparameter ctree (cutree 3 v))\n",
    "  (format t \"Cut tree: ~A ~%Class counts:~A ~%\" ctree\n",
    "           (loop for x across ctree \n",
    "                 when (= x 1) counting x into one \n",
    "                 when (= x 2) counting x into two \n",
    "                 when (= x 3) counting x into three \n",
    "                 finally (return (list (list 1 one) (list 2 two) (list 3 three)))))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " \n",
    "\n",
    "   (multiple-value-bind (workspace table)\n",
    "           (k-means 3 wine  :standardization t :random-state state)\n",
    "      ;;;; Now lets see how we did.\n",
    "      (print workspace  )\n",
    "      ;;;; #<CLML.HJS.K-MEANS::PROBLEM-WORKSPACE 3 Clusters (ID size):\n",
    "      ;;;;   ((0 51) (1 61) (2 66)) {10097B0003}>\n",
    "      ;;;; So the cluster distribution is ((0 51) (1 61) (2 66))\n",
    "      ;;;; from the dataset description in the .names file the\n",
    "      ;;;; actual distribution is: class 1 59\n",
    "\t  ;;;;                         class 2 71\n",
    "\t  ;;;;                         class 3 48\n",
    "      ;;;; Not too bad\n",
    "      ;;;; So what data is available from our clustering\n",
    "      ;;;; The cluster centroids:\n",
    "      (get-cluster-centroids workspace)\n",
    "      ;;;; ((0\n",
    "      ;;;;  . #(0.16444358896995892 0.8690954472537682 0.18637259499845618\n",
    "      ;;;;      0.5228924410811502 -0.07526046614302336 -0.9765754839938489\n",
    "      ;;;;      -1.2118292123465357 0.724021159373092 -0.7775131171790667\n",
    "      ;;;;      0.9388902409225227 -1.1615121632678649 -1.2887761433394702\n",
    "      ;;;;      -0.40594284143656034))\n",
    "      ;;;;      \n",
    "      ;;;;  (1\n",
    "      ;;;;  . #(0.8756272418170851 -0.30371957168664576 0.3180446264140999\n",
    "      ;;;;     -0.6626543948232362 0.5632992471916901 0.8740398997645874\n",
    "      ;;;;      0.9409846247986108 -0.5839425807469174 0.5801464167356171\n",
    "      ;;;;      0.1667181285145495 0.48236743459432646 0.7648958114081033\n",
    "      ;;;;      1.1550887659006268))\n",
    "      ;;;;  (2\n",
    "      ;;;;  . #(-0.9363618907319501 -0.3908632414705565 -0.4379655235785164\n",
    "      ;;;;      0.20840005437998074 -0.4624692470514986 -0.0531982454841447\n",
    "      ;;;;      0.06671557146906414 -0.019766389431300546 0.06460965992817365\n",
    "      ;;;;      -0.8795940625217612 0.45170767903647707 0.28892331536998617\n",
    "      ;;;;      -0.753898936464753))\n",
    "      ;;;;\n",
    "      ;;;;  Cluster points\n",
    "      (print (subseq (get-cluster-points workspace 0) 0 2))\n",
    "      ;;;; #(#(1.3911617440421347 1.5787117630892988 1.361367967705916 1.4987155594909907\n",
    "      ;;;;     -0.26196935775916486 -0.3916464788342626 -1.2707199546448487\n",
    "      ;;;;     1.5921313718328312 -0.4208878242554346 1.78662613110848 -1.5200903793122311\n",
    "      ;;;;     -1.4249282134945676 -0.5934862576893309)\n",
    "      ;;;;   #(0.2086431208420219 0.22705327972312908 0.012696271660310293\n",
    "      ;;;;     0.151234177571265 1.4184106667861087 -1.030776189881695 -1.350811364170554\n",
    "      ;;;;     1.351077174906624 -0.22870070674327575 1.8297614498435204\n",
    "      ;;;;     -1.5638403530401324 -1.396758819669375 0.2956638034293828))\n",
    "      ;;;;\n",
    "\n",
    "      )\n",
    "   )\n",
    "   (gethash (elt (get-cluster-points workspace 0) 0) table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion\n",
    "\n",
    "The iPython notebook and source for this tutorial can be found in the [clml.tutorials https://github.com/mmaul/clml.tutorials.git](https://github.com/mmaul/clml.tutorials.git) github repository.\n",
    "\n",
    "###Stay tuned to [clml.tutorials](https://mmaul.github.io/clml.tutorials/) blog or [RSS feed](https://mmaul.github.io/clml.tutorials/feed.xml) for more CLML tutorials..\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SBCL Lisp",
   "language": "lisp",
   "name": "lisp"
  },
  "language_info": {
   "name": "common-lisp",
   "version": "1.2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
